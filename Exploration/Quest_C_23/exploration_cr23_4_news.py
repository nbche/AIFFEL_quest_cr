# -*- coding: utf-8 -*-
"""Exploration_CR23_4_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1243P3Bh9_MxyCOeg30OuiWTs78TIJdcE
"""

!pip install sumy

!pip install summa

!pip install contractions

"""# 라이브러리 다운로드"""

from importlib.metadata import version
import nltk
import tensorflow
# import summa # This line is causing the error
import sumy # Use sumy instead of summa for text summarization
import pandas as pd
#------------------------------------#
import nltk   ## 데이터 전처리를 위한 라이브러리
nltk.download('stopwords')

import contractions
import numpy as np
import os
import re
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from bs4 import BeautifulSoup
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import urllib.request
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module='bs4')

#------------------------------------#
from tensorflow.keras.models import Model ## 모델링을 위한 라이브러리
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense
from tensorflow.keras.optimizers import Adam

"""# Step 1. 데이터 수집하기"""

import urllib.request
urllib.request.urlretrieve("https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv", filename="news_summary_more.csv")
data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')

print(data.sample(10))
print("-"*20)
print("데이터 프레임 정보:")
print(data.info())
print("-"*20)
print("\n기본 통계 요약:")
print(data.describe())

# headline 열에서 가장 긴 문자열의 행 번호 찾기
longest_headline_index = data['headlines'].str.len().idxmax()
longest_headline_row = data.iloc[longest_headline_index]
longest_headline = longest_headline_row['headlines']
longest_headline_length = len(longest_headline)

# text 열에서 가장 긴 문자열의 행 번호 찾기
longest_text_index = data['text'].str.len().idxmax()
longest_text_row = data.iloc[longest_text_index]
longest_text = longest_text_row['text']
longest_text_length = len(longest_text)

# 결과 출력
print(f"가장 긴 헤드라인 (행 번호 {longest_headline_index}, 길이 {longest_headline_length}):")
print(longest_headline)

print(f"\n가장 긴 텍스트 (행 번호 {longest_text_index}, 길이 {longest_text_length}):")
print(longest_text)
print("-"*20)
row_17829 = data.iloc[17829][0]
print(row_17829)

"""# Step 2. 데이터 전처리하기 (추상적 요약)

## 1) 데이터 정리하기
"""

print('전체 샘플수 :', (len(data)))
print('-'*20)
print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())  #중복 샘플과 NULL 값이 존재하는 샘플 제거
print('-'*20)
data.drop_duplicates(subset = ['text'], inplace=True) # inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다
print('전체 샘플수 :', (len(data)))
print('-'*20)
print(data.isnull().sum())
print('-'*20)
print('불용어 개수 :', len(stopwords.words('english') ))  #텍스트 정규화와 불용어 제거
print(stopwords.words('english'))

# 데이터 전처리 함수
def preprocess_sentence(sentence, remove_stopwords=True):
    sentence = sentence.lower() # 텍스트 소문자화
    sentence = BeautifulSoup(sentence, "lxml").text # <br />, <a href = ...> 등의 html 태그 제거
    sentence = re.sub(r'\([^)]*\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for
    sentence = re.sub('"','', sentence) # 쌍따옴표 " 제거
    # Fix: Use contractions.fix() to expand contractions
    sentence = contractions.fix(sentence) # 약어 정규화
    sentence = re.sub(r"'s\b","", sentence) # 소유격 제거. Ex) roland's -> roland
    sentence = re.sub("[^a-zA-Z]", " ", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환
    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah

    # 불용어 제거 (Text)
    if remove_stopwords:
        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)
    # 불용어 미제거 (Headlines)
    else:
        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)
    return tokens

clean_text = []
clean_text = data['text'].apply(lambda x: preprocess_sentence(x))
clean_headlines = []
clean_headlines = data['headlines'].apply(lambda x: preprocess_sentence(x),False)

# 전처리 후 출력
print("전처리 후 결과: ", clean_text[:5])
print('-'*20)
print("전처리 후 결과: ", clean_headlines[:5])

data['text'] = clean_text
data['headlines'] = clean_headlines

# 빈 값을 Null 값으로 변환
data.replace('', np.nan, inplace=True)
# Null 값 개수 확인하기
print(data.isnull().sum())
# Null 값 제거하기
data.dropna(axis=0, inplace=True)
print('전체 샘플수 :', (len(data)))

text_len = [len(s.split()) for s in data['text']]
headlines_len = [len(s.split()) for s in data['headlines']]

print(len(data["text"]))
print(len(data["headlines"]))

print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))
print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))
print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))
print('기사요약 최소 길이 : {}'.format(np.min(headlines_len)))
print('기사요약 최대 길이 : {}'.format(np.max(headlines_len)))
print('기사요약 평균 길이 : {}'.format(np.mean(headlines_len)))

plt.subplot(1,2,1)
plt.boxplot(text_len)
plt.title('Text')
plt.subplot(1,2,2)
plt.boxplot(headlines_len)
plt.title('Headlines')
plt.tight_layout()
plt.show()

plt.title('Text')
plt.hist(text_len, bins = 40)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

plt.title('Headlines')
plt.hist(headlines_len, bins = 40)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

# 기사 요약에는 시작 토큰과 종료 토큰을 추가한다.
data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)
data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')
data.head()

"""## 2) 훈련데이터와 테스트 데이터로 나누기"""

encoder_input = np.array(data['text']) # 인코더의 입력을 numpy 타입으로 저장
decoder_input = np.array(data['decoder_input']) # 디코더의 입력을 numpy 타입으로 저장
decoder_target = np.array(data['decoder_target']) # 디코더의 레이블을 numpy 타입으로 저장

# 훈련 데이터와 테스트 데이터로 분리
indices = np.arange(encoder_input.shape[0])
np.random.shuffle(indices)
print(indices)

# 위의 정수 시퀀스를 이용해 다시 데이터의 샘플 순서를 정의
encoder_input = encoder_input[indices]
decoder_input = decoder_input[indices]
decoder_target = decoder_target[indices]

# 8:2의 비율로 훈련 데이터와 테스트 데이터로 분리
n_of_val = int(len(encoder_input)*0.2)
print('테스트 데이터의 수 :', n_of_val)
print('-'*20)
encoder_input_train = encoder_input[:-n_of_val]
decoder_input_train = decoder_input[:-n_of_val]
decoder_target_train = decoder_target[:-n_of_val]

encoder_input_test = encoder_input[-n_of_val:]
decoder_input_test = decoder_input[-n_of_val:]
decoder_target_test = decoder_target[-n_of_val:]

print('훈련 데이터의 개수 :', len(encoder_input_train))
print('훈련 레이블의 개수 :', len(decoder_input_train))
print('테스트 데이터의 개수 :', len(encoder_input_test))
print('테스트 레이블의 개수 :', len(decoder_input_test))

"""## 3) 정수인코딩"""

src_tokenizer = Tokenizer() # 토크나이저 정의
src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성

threshold = 5    #등장 빈도수가 5회 미만인 단어 제외
total_cnt = len(src_tokenizer.word_index) # 단어의 수
rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.
for key, value in src_tokenizer.word_counts.items():
    total_freq = total_freq + value

    # 단어의 등장 빈도수가 threshold보다 작으면
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('단어 집합(vocabulary)의 크기 :', total_cnt)
print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))
print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))
print("단어 집합에서 희귀 단어의 비율:", (rare_cnt / total_cnt)*100)
print("전체 등장 빈도에서 희귀 단어 등장 빈도 비율:", (rare_freq / total_freq)*100)

src_vocab = 26000 ## 희귀 단어 제외시킨 단어 집단으로 크기 제한
src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 26,000으로 제한
src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성

# 텍스트 시퀀스를 정수 시퀀스로 변환
encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train)
encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)

# 잘 진행되었는지 샘플 출력
print(encoder_input_train[:3])

tar_tokenizer = Tokenizer()
tar_tokenizer.fit_on_texts(decoder_input_train)
print(decoder_input_train[:3])

threshold = 3
total_cnt = len(tar_tokenizer.word_index) # 단어의 수
rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.
for key, value in tar_tokenizer.word_counts.items():
    total_freq = total_freq + value

    # 단어의 등장 빈도수가 threshold보다 작으면
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('단어 집합(vocabulary)의 크기 :', total_cnt)
print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))
print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))
print("단어 집합에서 희귀 단어의 비율:", (rare_cnt / total_cnt)*100)
print("전체 등장 빈도에서 희귀 단어 등장 빈도 비율:", (rare_freq / total_freq)*100)

tar_vocab = 15000  ## 희귀 단어 제외시킨 단어 집단으로 크기 제한
tar_tokenizer = Tokenizer(num_words=tar_vocab) # 단어 집합의 크기를 15,000으로 제한
tar_tokenizer.fit_on_texts(decoder_input_train)
tar_tokenizer.fit_on_texts(decoder_target_train)

# 텍스트 시퀀스를 정수 시퀀스로 변환
decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)
decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)
decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)
decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)

# 잘 변환되었는지 확인
print('input')
print('input ',decoder_input_train[:5])
print('target')
print('decoder ',decoder_target_train[:5])

# Null 값의 자료 지우기

drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]
drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]

print('삭제할 훈련 데이터의 개수 :', len(drop_train))
print('삭제할 테스트 데이터의 개수 :', len(drop_test))

encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]
decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]
decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]

encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]
decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]
decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]

print('훈련 데이터의 개수 :', len(encoder_input_train))
print('훈련 레이블의 개수 :', len(decoder_input_train))
print('테스트 데이터의 개수 :', len(encoder_input_test))
print('테스트 레이블의 개수 :', len(decoder_input_test))

# 패딩하기
text_max_len = 60
headlines_max_len = 13

encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')
encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')
decoder_input_train = pad_sequences(decoder_input_train, maxlen=headlines_max_len, padding='post')
decoder_target_train = pad_sequences(decoder_target_train, maxlen=headlines_max_len, padding='post')
decoder_input_test = pad_sequences(decoder_input_test, maxlen=headlines_max_len, padding='post')
decoder_target_test = pad_sequences(decoder_target_test, maxlen=headlines_max_len, padding='post')

"""# Step 3. LTSM 모델 사용하기 (추상적 요약)

## 1) 모델 설계하기
"""

# LSTM 모델 구축

embedding_dim = 128  # Input dimension: Size of the vocabulary
latent_dim = 256  # Hidden state dimension of the LSTM

# 인코더
encoder_inputs = Input(shape=(text_max_len,))
# Fix: Provide output_dim argument to Embedding layer
enc_emb = Embedding(src_vocab + 1, embedding_dim, mask_zero=True)(
    encoder_inputs
)  # src_vocab + 1 is used to account for padding index (0)
encoder_lstm, state_h, state_c = LSTM(latent_dim, return_state=True)(enc_emb)
encoder_states = [state_h, state_c]

# 디코더
# Fix: Change the input shape to (headlines_max_len,)
decoder_inputs = Input(shape=(headlines_max_len,))
# Fix: Provide output_dim argument to Embedding layer
dec_emb = Embedding(tar_vocab + 1, embedding_dim, mask_zero=True)(
    decoder_inputs
)  # tar_vocab + 1 is used to account for padding index (0)
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
decoder_dense = Dense(
    tar_vocab + 1, activation="softmax"
)  # tar_vocab + 1 is used to account for padding index (0)
decoder_outputs = decoder_dense(decoder_outputs)

# 모델 정의
model_lstm = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model_lstm.summary()

# 모델 컴파일
model_lstm.compile(optimizer=Adam(), loss="sparse_categorical_crossentropy", metrics=["accuracy"])

# 모델 훈련
batch_size = 256
epochs = 50

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint = ModelCheckpoint(filepath='best_lstm_model.h5', monitor='val_loss', save_best_only=True)

history_lstm = model_lstm.fit([encoder_input_train, decoder_input_train], decoder_target_train,
                    epochs=epochs, batch_size=batch_size,
                    validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),
                    callbacks=[early_stopping, checkpoint])

import matplotlib.pyplot as plt

# 학습 결과 시각화
plt.plot(history_lstm.history['accuracy'])
plt.plot(history_lstm.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# 손실 그래프 시각화
plt.plot(history_lstm.history['loss'])
plt.plot(history_lstm.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""# Step 4. 실제 결과와 요약문 비교하기 (추상적 요약)"""

embedding_dim = 128  # Input dimension: Size of the vocabulary
latent_dim = 256  # Hidden state dimension of the LSTM

# 인코더 모델
encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)

# 디코더 상태 초기화를 위한 인풋 정의
decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

# 임베딩 및 LSTM 레이어
dec_emb2 = Embedding(tar_vocab + 1, embedding_dim, mask_zero=True)(decoder_inputs)
decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)
decoder_states2 = [state_h2, state_c2]
decoder_outputs2 = decoder_dense(decoder_outputs2)

# 디코더 모델
decoder_model = Model([decoder_inputs] + decoder_states_inputs,
                      [decoder_outputs2] + decoder_states2)

# 시퀀스를 텍스트로 변환
# Fix: Replace 'tokenizer' with 'tar_tokenizer'
reverse_word_map = dict(map(reversed, tar_tokenizer.word_index.items()))

def seq2summary(seq):
    summary = ''
    for item in seq:
        if item == 0:
            break
        if item in reverse_word_map:
            summary = summary + reverse_word_map[item] + ' '
    return summary.strip()

def decode_sequence(input_seq):
    # 인코더 상태
    states_value = encoder_model.predict(input_seq)

    # 디코더 시작 인풋으로는 start token을 사용
    target_seq = np.zeros((1, 1))
    # Fix: Replace 'tokenizer' with 'tar_tokenizer'
    target_seq[0, 0] = tar_tokenizer.word_index['start']

    stop_condition = False
    decoded_sentence = ''

    while not stop_condition:
        output_tokens, h, c = decoder_model.predict(
            [target_seq] + states_value)

        # 샘플링의 토큰 선택
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_word_map[sampled_token_index]
        decoded_sentence += ' ' + sampled_char

        # 종료 조건: 최대 길이에 도달하거나 stop token을 생성
        if (sampled_char == 'end' or
           len(decoded_sentence) > headlines_max_len):
            stop_condition = True

        # 업데이트된 타겟 시퀀스
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index

        # 업데이트된 디코더 상태
        states_value = [h, c]

    return decoded_sentence.strip()

# 테스트 데이터에 대해 모델 사용 예시
for seq_index in range(10):
    input_seq = encoder_input_test[seq_index: seq_index + 1]
    decoded_sentence = decode_sequence(input_seq)
    print("Input sequence:", data["headlines"][seq_index])
    print("Decoded summary:", decoded_sentence)

"""# Step 5. 어텐션 메커니즘 사용하기 (추상적 요약)"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 데이터 불러오기
data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')

# 데이터 분할
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
encoder_input_train = train_data['text'].values
decoder_input_train = train_data['headlines'].values
encoder_input_test = test_data['text'].values
decoder_input_test = test_data['headlines'].values

text_max_len = 60
summary_max_len = 13
top_k = 5000

# 토크나이저 초기화 및 텍스트 토큰화
tokenizer = Tokenizer(num_words=top_k)
tokenizer.fit_on_texts(np.concatenate((encoder_input_train, decoder_input_train), axis=0))

encoder_input_train = tokenizer.texts_to_sequences(encoder_input_train)
decoder_input_train = tokenizer.texts_to_sequences(decoder_input_train)
encoder_input_test = tokenizer.texts_to_sequences(encoder_input_test)
decoder_input_test = tokenizer.texts_to_sequences(decoder_input_test)

encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')
decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')
encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')
decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')

# Padding 된 시퀀스가 numpy 배열로 변환되었는지 및 dtype 확인
encoder_input_train = np.array(encoder_input_train, dtype=np.int32)
decoder_input_train = np.array(decoder_input_train, dtype=np.int32)
encoder_input_test = np.array(encoder_input_test, dtype=np.int32)
decoder_input_test = np.array(decoder_input_test, dtype=np.int32)

# 디코더 타겟 데이터 생성 및 변환
decoder_target_train = np.array([seq[1:] for seq in decoder_input_train], dtype=np.int32)
decoder_target_test = np.array([seq[1:] for seq in decoder_input_test], dtype=np.int32)
decoder_input_train = np.array([seq[:-1] for seq in decoder_input_train], dtype=np.int32)
decoder_input_test = np.array([seq[:-1] for seq in decoder_input_test], dtype=np.int32)

# 차원 확장으로 변환
decoder_target_train = np.expand_dims(decoder_target_train, axis=-1)
decoder_target_test = np.expand_dims(decoder_target_test, axis=-1)

assert encoder_input_train.dtype == np.int32
assert decoder_input_train.dtype == np.int32
assert encoder_input_test.dtype == np.int32
assert decoder_input_test.dtype == np.int32
assert decoder_target_train.dtype == np.int32
assert decoder_target_test.dtype == np.int32

from tensorflow.keras.layers import Layer

class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def call(self, inputs):
        decoder_lstm_output, encoder_lstm = inputs

        # score 계산
        score = Dot(axes=[2, 2])([decoder_lstm_output, encoder_lstm])
        score = Activation('softmax')(score)

        # context vector 계산
        context_vector = Dot(axes=[2, 1])([score, encoder_lstm])

        return context_vector, score

# 모델 정의 및 학습 부분 수정
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

embedding_dim = 128
latent_dim = 256

# 인코더
encoder_inputs = Input(shape=(text_max_len,))
enc_emb = Embedding(input_dim=top_k, output_dim=embedding_dim, mask_zero=True)(encoder_inputs)
encoder_lstm, state_h, state_c = LSTM(latent_dim, return_sequences=True, return_state=True)(enc_emb)
encoder_states = [state_h, state_c]

# 디코더
decoder_inputs = Input(shape=(summary_max_len-1,))
dec_emb_layer = Embedding(input_dim=top_k, output_dim=embedding_dim, mask_zero=True)
dec_emb = dec_emb_layer(decoder_inputs)

decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_lstm_output, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)

# 커스텀 Attention 레이어 사용
context_vector, attention_weights = AttentionLayer()([decoder_lstm_output, encoder_lstm])

# 디코더 출력과 컨텍스트 벡터 결합
decoder_combined_context = Concatenate(axis=-1)([context_vector, decoder_lstm_output])

# 최종 출력
output_layer = Dense(top_k, activation='softmax')
decoder_output = TimeDistributed(output_layer)(decoder_combined_context)

# 모델 정의
model = Model([encoder_inputs, decoder_inputs], decoder_output)
model.summary()

# 모델 컴파일
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# EarlyStopping 및 ModelCheckpoint 설정
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)

# 모델 학습
batch_size = 64
epochs = 20

history = model.fit([encoder_input_train, decoder_input_train], decoder_target_train,
                    epochs=epochs, batch_size=batch_size,
                    validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),
                    callbacks=[early_stopping, checkpoint])

# 학습 결과 시각화
plt.plot(history_lstm_attention.history['accuracy'])
plt.plot(history_lstm_attention.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_lstm_attention.history['loss'])
plt.plot(history_lstm_attention.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""# Step 6. Summa을 이용해서 추출적 요약해보기"""

import pandas as pd
from summa.summarizer import summarize

# CSV 파일에서 데이터 불러오기
data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')

# 텍스트 데이터를 포함하는 열의 이름을 지정
text_column = 'text'  # 열 이름을 실제 데이터에 맞게 변경하세요

# 텍스트 열의 데이터를 리스트로 변환
texts = data[text_column].tolist()

# 각 텍스트에 대해 요약을 수행하고 출력
for i, text in enumerate(texts):
    print(f'Summary of item {i+1}:')
    print(summarize(text, ratio=0.2))
    print('-' * 80)

# 각 텍스트에 대해 요약을 수행하고 출력
for i, text in enumerate(texts):
    print(f'Summary of item {i+1}:')
    print(summarize(text, words=10))
    print('-' * 80)

"""# Step 7. 결과 분석

1) 결과의 차이를 확인하기 위해, data 에 길이 제한을 두어 삭제하는 것은 일부러 수행치 않았음.

text 의 평균 길이는 30, 최대 길이는 60 이고 headline 의 평균 길이는 7, 최대 길이는 13 이었음

2) LSTM 모델의 경우 Attention Layer 를 포함하지 않았음

3) Attention Layer 가 추가하는 모델은 지속적인 에러로 인해 완료하지 못함

# Step 8. 회고

학습 시간 배분을 제대로 못하여, 내용을 완전히 숙지하지 못한 부분이 아쉽습니다. 자연어 처리를 하는 몇번 안되는 기회였기에 즐겁게 수행하였습니다.
"""

